{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e170a6c4",
   "metadata": {},
   "source": [
    "## Extract embeddings from BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "314995b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f38a56c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"The police is chasing a criminal on the run.\"\n",
    "\n",
    "# Tokenize the sentence and convert to input IDs\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens) # List of token ids\n",
    "\n",
    "# Tensor of token ids with batch size as 1; so shape will be torch.Size([1, 10])\n",
    "input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
    "\n",
    "# Get BERT embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "\n",
    "# Extract word embeddings from BERT outputs\n",
    "\n",
    "# Shape: [seq_len, hidden_size]\n",
    "input_embeddings = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "00d67f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 768])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04715f7",
   "metadata": {},
   "source": [
    "## Attention Mechanism\n",
    "\n",
    "Attention enables contextualized word embeddings by allowing the model to selectively focus on different parts of the input sequence when making predictions. Put simply, the attention mechanism allows the transformer to dynamically weigh the importance of different parts of the input sequence based on the current task and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "56c4e832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the input embeddings [batch_size, input_size,embedding_dimension]\n",
    "B, T, C = input_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4c36051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=1)\n",
    "out = wei @ input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ca68bf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = nn.Linear(C, C)\n",
    "query = nn.Linear(C, C)\n",
    "value = nn.Linear(C, C)\n",
    "\n",
    "k = key(input_embeddings)\n",
    "q = query(input_embeddings)\n",
    "v = value(input_embeddings)\n",
    "wei = q @ k.permute(0,2,1)\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=1)\n",
    "\n",
    "\n",
    "out = wei @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918820e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006e0c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(input_embed, block_type:str):\n",
    "    # Shape of the input embeddings [batch_size, input_size,embedding_dimension]\n",
    "    B, T, C = input_embed.shape\n",
    "    \n",
    "    key = nn.Linear(C, C)\n",
    "    query = nn.Linear(C, C)\n",
    "    value = nn.Linear(C, C)\n",
    "\n",
    "    k = key(input_embeddings)\n",
    "    q = query(input_embeddings)\n",
    "    v = value(input_embeddings)\n",
    "    wei = q @ k.permute(0,2,1) / C**0.5\n",
    "\n",
    "    tril = torch.tril(torch.ones(T,T))\n",
    "    \n",
    "    if block_type == \"decoder\":\n",
    "        wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "    \n",
    "    wei = F.softmax(wei, dim=1)\n",
    "\n",
    "    out = wei @ v\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049a4b64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "1b492a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 10])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a87e9883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 10])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c5eeda1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 768])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b7f73229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15,5))\n",
    "# sns.heatmap(out.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "5dd51c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-9.1074e-04,  1.0913e-02,  8.6040e-03,  ..., -2.1879e-04,\n",
       "          -1.4773e-02, -5.9723e-03],\n",
       "         [-2.2574e-02,  1.4292e-01,  1.8794e-01,  ..., -2.0085e-02,\n",
       "          -1.4592e-01, -4.8681e-02],\n",
       "         [ 1.6720e-02,  2.1470e-01,  3.2068e-01,  ..., -2.9511e-02,\n",
       "          -2.2448e-01, -7.7544e-02],\n",
       "         ...,\n",
       "         [ 8.5476e-02,  1.6527e-01,  2.0111e-01,  ..., -7.6960e-02,\n",
       "          -1.6512e-01, -5.5056e-02],\n",
       "         [ 2.0144e-01,  3.1347e-01,  2.0510e-01,  ..., -1.6995e-01,\n",
       "          -4.2555e-01, -1.9355e-02],\n",
       "         [ 6.9687e-01,  1.5490e+00,  9.0180e-01,  ..., -4.2238e-01,\n",
       "          -1.4563e+00, -7.3655e-01]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ec23ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5cbd67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_judge_env",
   "language": "python",
   "name": "llm_judge_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
