{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3be6dae9",
   "metadata": {},
   "source": [
    "## Extract embeddings from BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5c5ebd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "f38a56c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Sample sentence\n",
    "sentences = [\"The police is chasing a criminal on the run.\", \"The criminal is hiding in the police van.\"]\n",
    "\n",
    "# Tokenize the sentence and convert to input IDs\n",
    "input_ids = tokenizer(sentences, padding=True,return_tensors='pt')\n",
    "\n",
    "# Get BERT embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**input_ids)\n",
    "\n",
    "# # Extract word embeddings from BERT outputs\n",
    "\n",
    "# # Shape: [seq_len, hidden_size]\n",
    "input_embeddings = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdcebd4",
   "metadata": {},
   "source": [
    "# Attention Mechanism\n",
    "\n",
    "An attention mechanism is a key component in artificial intelligence and deep learning, particularly in models like transformers. It enables models to focus on specific parts of input data while making predictions or generating outputs. \n",
    "\n",
    "Imagine you're reading a long paragraph and trying to summarize it. Your attention is likely to focus more on important sentences or keywords rather than every single word. Similarly, in AI models, the attention mechanism helps the model assign different weights to different parts of the input data, giving more importance to relevant information.\n",
    "\n",
    "This mechanism allows the model to process sequences of data more effectively by selectively attending to the most relevant elements at each step of the computation, improving the model's performance in various tasks.\n",
    "\n",
    "<b>Breakdown</b>\n",
    "1. <b>Input</b>: Input data, such as a sequence of words in NLP or an image in computer vision, is typically transformed into embeddings or feature vectors that capture the data's semantic or structural information.\n",
    "2. <b> Query, Key, and Value </b>:  Input embeddings are further transformed into three sets of vectors: Query, Key, and Value. The transfomrations are linear projections followed by non-linear activation functions. \n",
    "3. <b> Attention Weights </b>: Attention weights indicate the importance or relevance of each element (token, pixel, etc.) in the input sequence. Computed using methods like (dot product, scaled dot product, etc.) between the Q and K.\n",
    "4. <b> Weighted Sum </b>: Once the attention weights are computed, they are used to calculate a weighted sum of the corresponding Value vectors. This represents the attended information, i.e., the parts of the input that are most relevant or important for the current context.\n",
    "\n",
    "Example:\n",
    "   - Q: What the token is looking for?\n",
    "   - K: Gist of what each token can offer.\n",
    "   - V: Actual content of each token.\n",
    "\n",
    "Imaginge browsing a streaming service. Your Query might be the genre like \"comedy\" or \"action-packed\". The movie titles, posters, and descriptions serve as Key. Actual film serve as the Value.\n",
    "\n",
    "<b>Multi-head attention</b>\n",
    "Attention mechanism is applied multiple times in parallel with different sets of Query, Key, and Value transformations. Multiple heads let us attend to several words. Each head can focus on specific ascpet of the input. Example: Subject, Object and the action. The outputs of multiple attention heads are concatenated or combined to provide diverse and richer representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "37c10e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the input embeddings [batch_size, input_size,embedding_dimension]\n",
    "B, T, C = input_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "4b4a3657",
   "metadata": {},
   "outputs": [],
   "source": [
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=1)\n",
    "out = wei @ input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "f40ca48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(input_embed, is_decoder:bool):\n",
    "    # Shape of the input embeddings [batch_size, input_size,embedding_dimension]\n",
    "    B, T, C = input_embed.shape\n",
    "    \n",
    "    key = nn.Linear(C, C)\n",
    "    query = nn.Linear(C, C)\n",
    "    value = nn.Linear(C, C)\n",
    "\n",
    "    k = key(input_embed)\n",
    "    q = query(input_embed)\n",
    "    v = value(input_embed)\n",
    "    wei = q @ k.permute(0,2,1) / C**0.5\n",
    "\n",
    "    tril = torch.tril(torch.ones(T,T))\n",
    "    \n",
    "    if is_decoder:\n",
    "        # in a decoder, the current token will not have access to future tokens. Ex: Generation.\n",
    "        # in an encoder, all the tokens will jointly attend to each other. Ex: Text Classification.\n",
    "        wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "    \n",
    "    wei = F.softmax(wei, dim=1)\n",
    "\n",
    "    out = wei @ v\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "9b5cbd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(input_embed, n_heads):\n",
    "    \"\"\"\n",
    "    Perform multi-head attention on input embeddings.\n",
    "\n",
    "    Args:\n",
    "    - input_embed: Input embeddings tensor of shape [batch_size, input_size, embedding_dimension].\n",
    "    - n_heads: Number of attention heads.\n",
    "\n",
    "    Returns:\n",
    "    - multi_head_output: Concatenated output of multi-head attention, shape [batch_size, input_size, n_heads * head_size].\n",
    "    \n",
    "    multi_head_output must pass through a Linear Layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    B, T, C = input_embed.shape\n",
    "    \n",
    "    head_size = C//n_heads\n",
    "    \n",
    "    multi_head_output = torch.tensor(())\n",
    "    for _ in range(n_heads):\n",
    "        key = nn.Linear(C, head_size)\n",
    "        query = nn.Linear(C, head_size)\n",
    "        value = nn.Linear(C, head_size)\n",
    "\n",
    "        k = key(input_embed)\n",
    "        q = query(input_embed)\n",
    "        v = value(input_embed)\n",
    "        wei = q @ k.permute(0,2,1) / C**0.5\n",
    "        tril = torch.tril(torch.ones(T,T))\n",
    "        wei = F.softmax(wei, dim=1)\n",
    "\n",
    "        out = wei @ v\n",
    "        multi_head_output = torch.cat((multi_head_output, out), -1)\n",
    "    print(\"Shape of each head\", out.shape)    \n",
    "    return multi_head_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "91f20256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of each head torch.Size([2, 12, 96])\n",
      "Shape of concatenated output torch.Size([2, 12, 768])\n"
     ]
    }
   ],
   "source": [
    "multi_head_output = multi_head_attention(input_embeddings, n_heads=8)\n",
    "print(\"Shape of concatenated output\", multi_head_output.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccdaa86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "ebecea10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_multi_head_attention(input_embed, n_heads):\n",
    "    \"\"\"\n",
    "    Perform masked multi-head attention on input embeddings.\n",
    "\n",
    "    Args:\n",
    "    - input_embed: Input embeddings tensor of shape [batch_size, input_size, embedding_dimension].\n",
    "    - n_heads: Number of attention heads.\n",
    "\n",
    "    Returns:\n",
    "    - masked_multi_head_output: Concatenated output of masked multi-head attention, shape [batch_size, input_size, n_heads * head_size].\n",
    "    \n",
    "    masked_multi_head_output must pass through a Linear Layer.\n",
    "    \"\"\"\n",
    "    B, T, C = input_embed.shape\n",
    "    \n",
    "    head_size = C//n_heads\n",
    "    \n",
    "    masked_multi_head_output = torch.tensor(())\n",
    "    for _ in range(n_heads):\n",
    "        key = nn.Linear(C, head_size)\n",
    "        query = nn.Linear(C, head_size)\n",
    "        value = nn.Linear(C, head_size)\n",
    "\n",
    "        k = key(input_embed)\n",
    "        q = query(input_embed)\n",
    "        v = value(input_embed)\n",
    "        wei = q @ k.permute(0,2,1) / C**0.5\n",
    "        tril = torch.tril(torch.ones(T,T))\n",
    "        \n",
    "        # Apply mask so that the current token will not have access to future tokens. \n",
    "        wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=1)\n",
    "\n",
    "        out = wei @ v\n",
    "        masked_multi_head_output = torch.cat((masked_multi_head_output, out), -1)\n",
    "        \n",
    "    return masked_multi_head_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "702df0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12, 768])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_multi_head_output = masked_multi_head_attention(input_embeddings, n_heads=8)\n",
    "masked_multi_head_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda97790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_judge_env",
   "language": "python",
   "name": "llm_judge_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
