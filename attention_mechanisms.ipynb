{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9468d34",
   "metadata": {},
   "source": [
    "## Extract embeddings from BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41691e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "f38a56c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Sample sentence\n",
    "sentences = [\"The police is chasing a criminal on the run.\", \"The criminal is hiding in the police van.\"]\n",
    "\n",
    "# Tokenize the sentence and convert to input IDs\n",
    "input_ids = tokenizer(sentences, padding=True,return_tensors='pt')\n",
    "\n",
    "# Get BERT embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**input_ids)\n",
    "\n",
    "# # Extract word embeddings from BERT outputs\n",
    "\n",
    "# # Shape: [seq_len, hidden_size]\n",
    "input_embeddings = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18069088",
   "metadata": {},
   "source": [
    "## Attention Mechanism\n",
    "\n",
    "Attention enables contextualized word embeddings by allowing the model to selectively focus on different parts of the input sequence when making predictions. Put simply, the attention mechanism allows the transformer to dynamically weigh the importance of different parts of the input sequence based on the current task and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "577aea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the input embeddings [batch_size, input_size,embedding_dimension]\n",
    "B, T, C = input_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "21e4a577",
   "metadata": {},
   "outputs": [],
   "source": [
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=1)\n",
    "out = wei @ input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "47c20e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(input_embed, is_decoder:bool):\n",
    "    # Shape of the input embeddings [batch_size, input_size,embedding_dimension]\n",
    "    B, T, C = input_embed.shape\n",
    "    \n",
    "    key = nn.Linear(C, C)\n",
    "    query = nn.Linear(C, C)\n",
    "    value = nn.Linear(C, C)\n",
    "\n",
    "    k = key(input_embed)\n",
    "    q = query(input_embed)\n",
    "    v = value(input_embed)\n",
    "    wei = q @ k.permute(0,2,1) / C**0.5\n",
    "\n",
    "    tril = torch.tril(torch.ones(T,T))\n",
    "    \n",
    "    if is_decoder:\n",
    "        # in a decoder, the current token will not have access to future tokens. Ex: Generation.\n",
    "        # in an encoder, all the tokens will jointly attend to each other. Ex: Text Classification.\n",
    "        wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "    \n",
    "    wei = F.softmax(wei, dim=1)\n",
    "\n",
    "    out = wei @ v\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5cbd67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_judge_env",
   "language": "python",
   "name": "llm_judge_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
